---
title: "Bayesian Changepoint Detection"
author: "Yuanhe Zhang"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Load library

```{r}
library(ggplot2)
library(hrbrthemes)
library(dplyr)
library(tidyr)
library(viridisLite)
library(viridis)
library(dplyr)
```

Load dataset

```{r}
load("C:/360Downloads/R/ST407/Assignment 2/a2data.Rdata")
```

## Question 1

## (a)

Generally,

Prior distribution of $k$,

$$
P(k)=
\begin{cases}
    \frac{k}{c} & k \leq \left\lfloor \frac{n-1}{2} \right\rfloor\\
    \frac{n-l}{c} & k>\left\lfloor \frac{n-1}{2} \right\rfloor\\
\end{cases}
$$

Prior distribution of $\mu_{1}$ and $\mu_{2}$,

$$
f^{prior}(\mu_{1},\mu_{2})=\frac{1}{\sqrt{20\pi}}exp(-\frac{\mu_{1}^{2}}{20})\frac{1}{\sqrt{20\pi}}exp(-\frac{\mu_{2}^{2}}{20})=\frac{1}{20\pi}exp(-\frac{\mu_{1}^{2}+\mu_{2}^{2}}{20})
$$

And Likelihood:

$$
l(\textbf{y}|\mu_{1},\mu_{2},k)=\prod_{p=1}^{k}\phi_{\mu_{1},1}(y_{p})\prod_{q=k+1}^{n}\phi_{\mu_2,1}(y_{q})=\prod_{p=1}^{k}\frac{1}{\sqrt{2\pi}}exp(-\frac{(y_{p}-\mu_{1})^{2}}{2})\prod_{q=k+1}^{n}\frac{1}{\sqrt{2\pi}}exp(-\frac{(y_{q}-\mu_{2})^{2}}{2})
$$

Set $k\leq \left\lfloor \frac{n-1}{2} \right\rfloor$, then prior distribution of $k$ is $P(k)=\frac{k}{c}$,

then the joint posterior is,

$$
\begin{split}
f_{1}^{(post)}(\mu_{1},\mu_{2},k|\textbf{y})\\\propto f^{prior}(\mu_{1},\mu_{2})*P(k)*l(\textbf{y}|\mu_{1},\mu_{2},k)\\=\frac{k}{c}\frac{1}{\sqrt{20\pi}}exp(-\frac{\mu_{1}^{2}}{20})\frac{1}{\sqrt{20\pi}}exp(-\frac{\mu_{2}^{2}}{20})\prod_{p=1}^{k}\frac{1}{\sqrt{2\pi}}exp(-\frac{(y_{p}-\mu_{1})^{2}}{2})\prod_{q=k+1}^{n}\frac{1}{\sqrt{2\pi}}exp(-\frac{(y_{q}-\mu_{2})^{2}}{2})
\\=\frac{k}{20\pi c}exp(-\frac{\mu_{1}^{2}+\mu_{2}^{2}}{20})(\frac{1}{\sqrt{2\pi}})^{k}\prod_{p=1}^{k}exp(-\frac{(y_{p}-\mu_{1})^{2}}{2})(\frac{1}{\sqrt{2\pi}})^{n-k}\prod_{q=k+1}^{n}exp(-\frac{(y_{q}-\mu_{2})^{2}}{2})
\\=\frac{k}{20\pi c}(\frac{1}{\sqrt{2\pi}})^{n}exp(-\frac{\mu_{1}^{2}+\mu_{2}^{2}}{20})\prod_{p=1}^{k}exp(-\frac{(y_{p}-\mu_{1})^{2}}{2})\prod_{q=k+1}^{n}exp(-\frac{(y_{q}-\mu_{2})^{2}}{2})
\\=\frac{k}{10c(2\pi)^{\frac{n}{2}+1}}exp(-\frac{\mu_{1}^{2}+\mu_{2}^{2}}{20})\prod_{p=1}^{k}exp(-\frac{(y_{p}-\mu_{1})^{2}}{2})\prod_{q=k+1}^{n}exp(-\frac{(y_{q}-\mu_{2})^{2}}{2})
\\\propto k*exp(-\frac{\mu_{1}^{2}+\mu_{2}^{2}}{20})\prod_{p=1}^{k}exp(-\frac{(y_{p}-\mu_{1})^{2}}{2})\prod_{q=k+1}^{n}exp(-\frac{(y_{q}-\mu_{2})^{2}}{2})
\end{split}
$$

We write the joint posterior up to a normalising constant $Z$(define later),

$$
f_{1}^{(post)}(\mu_{1},\mu_{2},k|\textbf{y})=\frac{k*exp(-\frac{\mu_{1}^{2}+\mu_{2}^{2}}{20})\prod_{p=1}^{k}exp(-\frac{(y_{p}-\mu_{1})^{2}}{2})\prod_{q=k+1}^{n}exp(-\frac{(y_{q}-\mu_{2})^{2}}{2})}{Z}
$$

Set $k> \left\lfloor \frac{n-1}{2} \right\rfloor$, then prior distribution of $k$ is $P(k)=\frac{n-k}{c}$,

then the joint posterior is,

$$
\begin{split}
f_{2}^{(post)}(\mu_{1},\mu_{2},k|\textbf{y})\\\propto f^{prior}(\mu_{1},\mu_{2})*P(k)*l(\textbf{y}|\mu_{1},\mu_{2},k)\\=\frac{n-k}{c}\frac{1}{\sqrt{20\pi}}exp(-\frac{\mu_{1}^{2}}{20})\frac{1}{\sqrt{20\pi}}exp(-\frac{\mu_{2}^{2}}{20})\prod_{p=1}^{k}\frac{1}{\sqrt{2\pi}}exp(-\frac{(y_{p}-\mu_{1})^{2}}{2})\prod_{q=k+1}^{n}\frac{1}{\sqrt{2\pi}}exp(-\frac{(y_{q}-\mu_{2})^{2}}{2})\\=\frac{n-k}{20\pi c}exp(-\frac{\mu_{1}^{2}+\mu_{2}^{2}}{20})\prod_{p=1}^{k}\frac{1}{\sqrt{2\pi}}exp(-\frac{(y_{p}-\mu_{1})^{2}}{2})\prod_{q=k+1}^{n}\frac{1}{\sqrt{2\pi}}exp(-\frac{(y_{q}-\mu_{2})^{2}}{2})
\\=\frac{n-k}{10c(2\pi)^{\frac{n}{2}+1}}exp(-\frac{\mu_{1}^{2}+\mu_{2}^{2}}{20})\prod_{p=1}^{k}exp(-\frac{(y_{p}-\mu_{1})^{2}}{2})\prod_{q=k+1}^{n}exp(-\frac{(y_{q}-\mu_{2})^{2}}{2})
\\\propto  (n-k)*exp(-\frac{\mu_{1}^{2}+\mu_{2}^{2}}{20})\prod_{p=1}^{k}exp(-\frac{(y_{p}-\mu_{1})^{2}}{2})\prod_{q=k+1}^{n}exp(-\frac{(y_{q}-\mu_{2})^{2}}{2})
\end{split}
$$

We write the joint posterior up to a normalising constant $Z$(define later),

$$
f_{2}^{(post)}(\mu_{1},\mu_{2},k|\textbf{y}) = \frac{(n-k)*exp(-\frac{\mu_{1}^{2}+\mu_{2}^{2}}{20})\prod_{p=1}^{k}exp(-\frac{(y_{p}-\mu_{1})^{2}}{2})\prod_{q=k+1}^{n}exp(-\frac{(y_{q}-\mu_{2})^{2}}{2})}{Z}
$$

Lastly, we define the normalising constant $Z$ as,

$$
Z=10c(2\pi)^{\frac{n}{2}+1}\sum_{k}P^{(prior)}(k)\int_{\mu_{1}}\int_{\mu_{2}} f^{prior}(\mu_{1},\mu_{2})l(\textbf{y}|\mu_{1},\mu_{2},k) \,d\mu_{1}\mu_{2}
$$

Finally, we write our joint posterior of $\mu_{1},\mu_{2},k$ given a sequence of observations $\textbf{y}$ up to a noemalising constant $Z$ as,

$$
f^{(post)}(\mu_{1},\mu_{2},k|\textbf{y}) = 
\begin{cases}
    \frac{k*exp(-\frac{\mu_{1}^{2}+\mu_{2}^{2}}{20})\prod_{p=1}^{k}exp(-\frac{(y_{p}-\mu_{1})^{2}}{2})\prod_{q=k+1}^{n}exp(-\frac{(y_{q}-\mu_{2})^{2}}{2})}{Z} & k\leq \left\lfloor \frac{n-1}{2} \right\rfloor \\
    \frac{(n-k)*exp(-\frac{\mu_{1}^{2}+\mu_{2}^{2}}{20})\prod_{p=1}^{k}exp(-\frac{(y_{p}-\mu_{1})^{2}}{2})\prod_{q=k+1}^{n}exp(-\frac{(y_{q}-\mu_{2})^{2}}{2})}{Z} & k> \left\lfloor \frac{n-1}{2} \right\rfloor\\
\end{cases}
$$ where $Z=10c(2\pi)^{\frac{n}{2}+1} \sum_{k}P^{(prior)}(k)\int_{\mu_{1}}\int_{\mu_{2}} f^{prior}(\mu_{1},\mu_{2})l(\textbf{y}|\mu_{1},\mu_{2},k) \,d\mu_{1}\mu_{2}$.

## (b)

First, we compute the full conditional distribution of $\mu_{1}$. For simplicity, whether it is $k$ or $n-k$ in the joint posterior will be removed so we can write two posterior corresponding to different range of $k$ to just one posterior for the full conditional distribution of $\mu_{1}$ (same for $\mu_{2}$),

$$
\begin{split}
f(\mu_{1}|\textbf{y},\mu_{2},k)\\\propto exp(-\frac{\mu_{1}^{2}}{20})\prod_{p=1}^{k}exp(-\frac{(y_{p}-\mu_{1})^{2}}{2})\\=exp(-\frac{\mu_{1}^{2}}{20})exp(-\frac{\sum_{p=1}^{k}(y_{p}-\mu_{1})^{2}}{2})\\=exp(-\frac{\mu_{1}^{2}}{20})exp(-\frac{\sum_{p=1}^{k}(y_{p}^{2}-2y_{p}\mu_{1}+\mu_{1}^{2})}{2})\\=exp(-\frac{\mu_{1}^{2}}{20})exp(-\frac{\sum_{p=1}^{k}y_{p}^{2}-2\sum_{p=1}^{k}y_{p}\mu_{1}+k\mu_{1}^{2}}{2})\\\propto exp(-\frac{\mu_{1}^{2}}{20})exp(\frac{2\sum_{p=1}^{k}y_{p}\mu_{1}-k\mu_{1}^{2}}{2})\\=exp(\frac{-\frac{\mu_{1}^{2}}{10}}{2}+\frac{2\sum_{p=1}^{k}y_{p}\mu_{1}-k\mu_{1}^{2}}{2})\\=exp(-\frac{-2\sum_{p=1}^{k}y_{p}\mu_{1}+(k+\frac{1}{10})\mu_{1}^{2}}{2})\\=exp(-\frac{\mu_{1}^{2}-2\frac{\sum_{p=1}^{k}y_{p}}{(k+\frac{1}{10})}\mu_{1}}{\frac{2}{k+\frac{1}{10}}})\\=exp(-\frac{(\mu_{1}-\frac{\sum_{p=1}^{k}y_{p}}{k+\frac{1}{10}})^{2}}{\frac{2}{k+\frac{1}{10}}})*const
\end{split}
$$

therefore we can obtain that,

$$
\mu_{1}|\textbf{y},\mu_{2},k \sim N(\frac{\sum_{p=1}^{k}y_{p}}{k+\frac{1}{10}},\frac{1}{k+\frac{1}{10}})
$$

First, we compute the full conditional distribution of $\mu_{2}$.

$$
\begin{split}
f(\mu_{2}|\textbf{y},\mu_{1},k)\\\propto
exp(-\frac{\mu_{2}^{2}}{20})\prod_{q=k+1}^{n}exp(-\frac{(y_{q}-\mu_{2})^{2}}{2})\\\propto
exp(-\frac{\mu_{2}^{2}}{20})exp(\frac{2\sum_{q=k+1}^{n}y_{q}\mu_{2}-(n-k)\mu_{2}^{2}}{2})\\=
exp(-\frac{\frac{\mu_{2}^{2}}{10}}{2}-\frac{-2\sum_{q=k+1}^{n}y_{q}\mu_{2}+(n-k)\mu_{2}^{2}}{2})\\=
exp(-\frac{-2\sum_{q=k+1}^{n}y_{q}\mu_{2}+(n-k+\frac{1}{10})\mu_{2}^{2}}{2})\\=
exp(-\frac{\mu_{2}^{2}-2\frac{\sum_{q=k+1}^{n}y_{q}}{(n-k+\frac{1}{10})}\mu_{2}}{\frac{2}{n-k+\frac{1}{10}}})\\=
exp(-\frac{(\mu_{2}-\frac{\sum_{q=k+1}^{n}y_{q}}{n-k+\frac{1}{10}})^{2}}{\frac{2}{n-k+\frac{1}{10}}})*const
\end{split}
$$

therefore we can obtain that,

$$
\mu_{2}|\textbf{y},\mu_{1},k \sim N(\frac{\sum_{q=k+1}^{n}y_{q}}{n-k+\frac{1}{10}},\frac{1}{n-k+\frac{1}{10}})
$$

Finally, we compute the full conditional distribution of $k$.

Set $k\leq \left\lfloor \frac{n-1}{2} \right\rfloor$,

$$
\begin{split}
f_{1}(k|\textbf{y},\mu_{1},\mu_{2})\propto k*\prod_{p=1}^{k}exp(-\frac{(y_{p}-\mu_{1})^{2}}{2})\prod_{q=k+1}^{n}exp(-\frac{(y_{q}-\mu_{2})^{2}}{2})
\end{split}
$$

Set $k> \left\lfloor \frac{n-1}{2} \right\rfloor$,

$$
f_{2}(k|\textbf{y},\mu_{1},\mu_{2})\propto (n-k)*\prod_{p=1}^{k}exp(-\frac{(y_{p}-\mu_{1})^{2}}{2})\prod_{q=k+1}^{n}exp(-\frac{(y_{q}-\mu_{2})^{2}}{2})
$$

Define a normalising constant $C$ as,

$$
C=\sum_{k}\left\{ \left(\mathbf{1}_{\{k\leq \left\lfloor \frac{n-1}{2} \right\rfloor\}}*k+\mathbf{1}_{\{k> \left\lfloor \frac{n-1}{2} \right\rfloor\}}*(n-k)\right)*\prod_{p=1}^{k}exp(-\frac{(y_{p}-\mu_{1})^{2}}{2})\prod_{q=k+1}^{n}exp(-\frac{(y_{q}-\mu_{2})^{2}}{2})  \right\}
$$

where $\mathbf{1}_{\{\cdot \}}$ is the indicator function.

then we can obtain the full conditional distribution of $k$.

$$
f(k|\mu_{1},\mu_{2},\textbf{y}) = 
\begin{cases}
    \frac{k*\prod_{p=1}^{k}exp(-\frac{(y_{p}-\mu_{1})^{2}}{2})\prod_{q=k+1}^{n}exp(-\frac{(y_{q}-\mu_{2})^{2}}{2})}{C} & k\leq \left\lfloor \frac{n-1}{2} \right\rfloor \\
    \frac{(n-k)*\prod_{p=1}^{k}exp(-\frac{(y_{p}-\mu_{1})^{2}}{2})\prod_{q=k+1}^{n}exp(-\frac{(y_{q}-\mu_{2})^{2}}{2})}{C} & k> \left\lfloor \frac{n-1}{2} \right\rfloor\\
\end{cases}
$$

And the full conditional distribution of $\mu_{1}$, $\mu_{2}$.

$$
\begin{split}
\mu_{1}|\textbf{y},\mu_{2},k \sim N(\frac{\sum_{p=1}^{k}y_{p}}{k+\frac{1}{10}},\frac{1}{k+\frac{1}{10}})\\
\mu_{2}|\textbf{y},\mu_{1},k \sim N(\frac{\sum_{q=k+1}^{n}y_{q}}{n-k+\frac{1}{10}},\frac{1}{n-k+\frac{1}{10}})
\end{split}
$$

## (c)

we use the function provided in the assignments sheet and let me give an explanation why it works,

In the part of code: log(c(1:(floor((n-1)/2)),n-(ceiling((n/2)+1):n-1))), it allocated each $k$ corresponding prior without normalising constant $c$, also this is same as the $k$ and $n-k$ in $f(k|\mu_{1},\mu_{2},\textbf{y})$ we derived above refers to different range of $k$.

then in the for loop part, the code aims to give each corresponding $k$ $log(\frac{\prod_{p=1}^{k}exp(-\frac{(y_{p}-\mu_{1})^{2}}{2})}{\prod_{p=1}^{k}exp(-\frac{(y_{p}-\mu_{2})^{2}}{2})})$. This computation is alternate to $log(\prod_{p=1}^{k}exp(-\frac{(y_{p}-\mu_{1})^{2}}{2})\prod_{q=k+1}^{n}exp(-\frac{(y_{q}-\mu_{2})^{2}}{2}))$ because here we have fixed $\mu_{1}$ and $\mu_{2}$, and observed data $\textbf{y}$, then $\prod_{q=1}^{n}exp(-\frac{(y_{q}-\mu_{2})^{2}}{2})$ is a constant, we can assume $K=\prod_{q=1}^{n}exp(-\frac{(y_{q}-\mu_{2})^{2}}{2})$, then,

$$
\frac{1}{\prod_{p=1}^{k}exp(-\frac{(y_{p}-\mu_{2})^{2}}{2})}=\frac{\prod_{q=k+1}^{n}exp(-\frac{(y_{q}-\mu_{2})^{2}}{2})}{K}
$$

so

$$
\frac{\prod_{p=1}^{k}exp(-\frac{(y_{p}-\mu_{1})^{2}}{2})}{\prod_{p=1}^{k}exp(-\frac{(y_{p}-\mu_{2})^{2}}{2})}=\frac{\prod_{p=1}^{k}exp(-\frac{(y_{p}-\mu_{1})^{2}}{2})\prod_{q=k+1}^{n}exp(-\frac{(y_{q}-\mu_{2})^{2}}{2})}{K}
$$

$$
log(\frac{\prod_{p=1}^{k}exp(-\frac{(y_{p}-\mu_{1})^{2}}{2})}{\prod_{p=1}^{k}exp(-\frac{(y_{p}-\mu_{2})^{2}}{2})})=log(\frac{\prod_{p=1}^{k}exp(-\frac{(y_{p}-\mu_{1})^{2}}{2})\prod_{q=k+1}^{n}exp(-\frac{(y_{q}-\mu_{2})^{2}}{2})}{K})
$$

$$
log(\frac{\prod_{p=1}^{k}exp(-\frac{(y_{p}-\mu_{1})^{2}}{2})}{\prod_{p=1}^{k}exp(-\frac{(y_{p}-\mu_{2})^{2}}{2})})=log(\prod_{p=1}^{k}exp(-\frac{(y_{p}-\mu_{1})^{2}}{2})\prod_{q=k+1}^{n}exp(-\frac{(y_{q}-\mu_{2})^{2}}{2}))-log(K)
$$

$$
e^{log(\frac{\prod_{p=1}^{k}exp(-\frac{(y_{p}-\mu_{1})^{2}}{2})}{\prod_{p=1}^{k}exp(-\frac{(y_{p}-\mu_{2})^{2}}{2})})}=\prod_{p=1}^{k}exp(-\frac{(y_{p}-\mu_{1})^{2}}{2})\prod_{q=k+1}^{n}exp(-\frac{(y_{q}-\mu_{2})^{2}}{2})\cdot \frac{1}{K}
$$

for the $\frac{1}{K}$ in this form, we can normalise it in the later steps because it is a multiplier.

And we minus max(logpost.k) in order to avoid NA output here for numerical reasons.

Lastly, normalise by post = post / sum (post) to get the exact posterior distribution of $k$. And use sample(n-1,size =1,prob=post) to sample from this distribution. So this function exactly this function returns samples from the distribution which we derived in part b.

```{r}
gibbs.k = function(mu1,mu2,y){
n = length (y)

logprior.k = log(c(1:(floor((n-1)/2)),n-(ceiling((n/2)+1):n-1)))
logpost.k = logprior.k

cr = 0
for (i in 1:(n-1)){
   cr = cr + dnorm (y[i], mean = mu1 , sd =1 , log = TRUE )
   cr = cr - dnorm (y[i], mean = mu2 , sd =1 , log = TRUE )
   logpost.k[i] = logpost.k[i] + cr
}

post = exp(logpost.k - max(logpost.k))
post = post / sum (post)

sample(n-1 , size =1 , prob = post )
}
```

```{r}
pmix.gibbs=function(k,mu1,mu2,y,t){
  n = length (y)
  r=array(NA,c(t+1,3))
  r[1,]=c(k,mu1,mu2)
  for (i in 1:t) {
    r[i+1,2]=rnorm(1,mean=(sum(y[1:r[i,1]])/(r[i,1]+0.1)),sd=(1/(r[i,1]+0.1)))
    r[i+1,3]=rnorm(1,mean=(sum(y[(r[i,1]+1):n])/(n-r[i,1]+0.1)),sd=(1/(n-r[i,1]+0.1)))
    r[i+1,1]=gibbs.k(r[i+1,2],r[i+1,3],y)
  }
  r
}
```

## (d)

## i.

Run Gibbs sampler on the first data series with initial values $X^{(0)}=(k^{(0)},\mu_{1}^{(0)},\mu_{2}^{(0)})=(1,1,1)$ and generate 10000 samples,

```{r}
l1=pmix.gibbs(1,1,1,dataset1,10000)
```

## ii.

First, we look at the trace of changepoint location for $k$,

```{r}
plot(l1[,1],type = 'l',col='blue',main = 'Trace of changepoint location k',
     xlab = 'Iteration',ylab = 'Value of k')
```

Also, we can plot the estimated posterior distribution of $k$,

```{r}
dk1=as.data.frame(table(l1[,1]))
plot(as.vector(dk1$Var1),as.vector(dk1$Freq/sum(dk1$Freq)),type='h',
     xlim=c(0,30),ylim = c(0,0.8),lwd=2,col='blue',
     main = 'Estimated posterior distribution of k',xlab = 'Value of k',
     ylab = 'density')
points(as.vector(dk1$Var1),as.vector(dk1$Freq/sum(dk1$Freq)),col='blue',lwd=1)
```

Next, let us visualize the density plot and trace plot of $\mu_{1}$ and $\mu_{2}$,

```{r}
hist(l1[,2],freq = FALSE, ,breaks=50, col = 'lightblue',
     main = 'Estimated posterior distribution of mu_1',xlab = 'mu_1')
lines(density(l1[,2]),col='red',lwd=2)

plot(l1[,2], col = 'blue',main = 'Trace of mu_1',xlab = 'Iteration',
     ylab = 'Value of mu_1',type='l')
```

```{r}
hist(l1[,3],freq = FALSE, ,breaks=50, col = 'lightblue',
     main = 'Estimated posterior distribution of mu_2',xlab = 'mu_2')
lines(density(l1[,3]),col='red',lwd=2)

plot(l1[,3], col = 'blue',main = 'Trace of mu_2',xlab = 'Iteration',
     ylab = 'Value of mu_2',type='l')
```

Then, we calculate the effective sample size of our output using the autocorrelation of the changepoint location,

```{r}
#get the autocorrelation of changepoint location k
auto_cf_l1_k = acf(l1[,1],plot = FALSE)
#get the first order sample autocorrelation of changepoint location k
acf1 = auto_cf_l1_k$acf[2]
acf1
```

so we can obtain that $\rho(X_{1}^{(t-1)},X_{1}^{(t)})=0.05868507$, the by the formula of ESS

$$
T_{ESS} = \frac{1-\rho}{1+\rho}\cdot T
$$

```{r}
T_ESS = round(10000*((1-acf1)/(1+acf1)))
print(paste("Thus the effective sample size is",T_ESS))
```

This ESS size is big enough to converge. Therefore, we don't need to run the sampler again for a longer period because the effective sample size is high. This means comparing to i.i.d. sampling, there are not much information loss in the sample from the Markov Chain.

Alternately, we can assess the convergence by cumulative mean average, and the plots showed that the average converges very fast.

```{r}
par(mfrow = c(2, 2))
cum_avg1 <- cummean(l1[,1])
cum_avg2 <- cummean(l1[,2])
cum_avg3 <- cummean(l1[,3])

plot(cum_avg1,type='l',xlab = 'sample',ylab = 'cumulative average',
     main = 'Cumulative means of k')
plot(cum_avg2,type='l',xlab = 'sample',ylab = 'cumulative average',
     main = 'Cumulative means of mu_1')
plot(cum_avg3,type='l',xlab = 'sample',ylab = 'cumulative average',
     main = 'Cumulative means of mu_2')
```

## iii.

Compare the prior and estimated posterior distributions of the three parameters of interest ($k,\mu_{1},\mu_{2}$).

First, write a function to get the prior distribution of $k$,

```{r}
prior.k=function(y){
  n=length(y)
  logprior.k = log(c(1:(floor((n-1)/2)),n-(ceiling((n/2)+1):n-1)))
  prior=logprior.k/sum(logprior.k)
  prior
}
```

then plot the posterior vs prior of $k$, the prior of $k$ is also discrete but we plot it as a line with points in order to visualize more clearly,

```{r fig.height=5,fig.width=7}
plot(as.vector(dk1$Var1),as.vector(dk1$Freq/sum(dk1$Freq)),type='h',
     xlim=c(0,30),ylim = c(0,0.8),lwd=2,col='blue',
     main = 'Posterior vs prior of k',xlab = 'Value of k',ylab = 'density')
points(as.vector(dk1$Var1),as.vector(dk1$Freq/sum(dk1$Freq)),col='blue',lwd=1)
lines(prior.k(dataset1),col='green')
points(prior.k(dataset1),col='green')
legend(x = "topright",          
       legend = c("prior", "posterior"),  
       pch = c(1,1),           
       col = c("green", "blue"),          
       lwd = 2)
```

plot the posterior vs prior of $\mu_{1}$,

```{r fig.height=5,fig.width=12}
#generate prior of mu_1
y=rnorm(10000,0,sqrt(10))

# Build dataset with different distributions
data1_1 <- data.frame(
  type = c( rep("posterior", 10001), rep("prior", 10000) ),
  value = c(l1[,2], y)
)

p1_1 <- ggplot(data1_1, aes(x=value, fill=type, color=type)) +
    geom_histogram(aes(y=after_stat(density)), alpha=0.6, 
                   position = 'identity',binwidth = .1) +
    scale_color_manual(values=c("#999999", "#E69F00"))+
    scale_fill_manual(values=c("#999999", "#E69F00"))+
    labs(title="prior vs density posterior plot for mu1",x="Value of mu_1", 
         y = "Density")+
    theme_classic()
p1_1
```

plot the posterior vs prior of $\mu_{2}$,

```{r fig.height=5,fig.width=12}
# Build dataset with different distributions
data1_2 <- data.frame(
  type = c( rep("posterior", 10001), rep("prior", 10000) ),
  value = c(l1[,3], y)
)

p1_2 <- ggplot(data1_2, aes(x=value, fill=type, color=type)) +
    geom_histogram(aes(y=..density..), alpha=0.6, position = 'identity',
                   binwidth = .08) +
    scale_color_manual(values=c("#69b3a2", "#404080"))+
    scale_fill_manual(values=c("#69b3a2", "#404080"))+
    labs(title="prior vs posterior density plot for mu2",x="Value of mu_2", 
         y = "Density")+
    theme_classic()
p1_2
```

From these plots, we could clearly see that all posteriors of these three parameters of interest have some differences with their corresponding prior distribution. This means that definitely there is some information in the data and through gibbs sampler we learn more information to update our beliefs.

The posterior distribution of three parameters of interest are very concentrated so we can think of that the change point location is around 11 or 12.

posterior mean and variance of each of the parameters of interest,

```{r}
#posterior mean
mk=mean(l1[,1])
m1=mean(l1[,2])
m2=mean(l1[,3])

#posterior variance
vk=var(l1[,1])
v1=var(l1[,2])
v2=var(l1[,3])

mean_int = c(mk,m1,m2)
var_int = c(vk,v1,v2)
param=c('k','mu_1','mu_2')

param_1 <- data.frame('parameter'=param, "posterior mean"=mean_int, 
                      "posterior variance" = var_int)
param_1
```

## (e)

Repeat (d) on the second data series, use same question index with (d), and comment the differences at last.

## i.

Run Gibbs sampler on the second data series with initial values $X^{(0)}=(k^{(0)},\mu_{1}^{(0)},\mu_{2}^{(0)})=(1,1,1)$ and generate 10000 samples,

```{r}
l2=pmix.gibbs(1,1,1,dataset2,10000)
```

## ii.

First, we look at the trace of changepoint location for $k$,

```{r}
plot(l2[,1],type = 'l',col='red',main = 'Trace of changepoint location k',
     xlab = 'Iteration',ylab = 'Value of k')
```

Also, we can plot the estimated posterior distribution of $k$,

```{r}
dk2=as.data.frame(table(l2[,1]))
plot(as.vector(dk2$Var1),as.vector(dk2$Freq/sum(dk2$Freq)),type='h',
     xlim=c(0,150),ylim = c(0,0.045),lwd=2,col='blue',
     main = 'Estimated posterior distribution of k',xlab = 'Value of k',
     ylab = 'density')
points(as.vector(dk2$Var1),as.vector(dk2$Freq/sum(dk2$Freq)),col='blue',lwd=1)
```

Next, let us visualize the density plot and trace plot of $\mu_{1}$ and $\mu_{2}$,

```{r}
hist(l2[,2],freq = FALSE, ,breaks=100, col = 'lightblue',
     main = 'Estimated posterior distribution of mu_1',xlab = 'mu_1')
lines(density(l2[,2]),col='red',lwd=2)

plot(l2[,2], col = 'red',main = 'Trace of mu_1',xlab = 'Iteration',
     ylab = 'Value of mu_1',type='l')
```

```{r}
hist(l2[,3],freq = FALSE, ,breaks=100, col = 'lightblue',
     main = 'Estimated posterior distribution of mu_2',xlab = 'mu_2')
lines(density(l2[,3]),col='red',lwd=2)

plot(l2[,3], col = 'red',main = 'Trace of mu_2',xlab = 'Iteration',
     ylab = 'Value of mu_2',type='l')
```

Then, we calculate the effective sample size of our output using the autocorrelation of the changepoint location,

```{r}
#get the autocorrelation of changepoint location k
auto_cf_l2_k = acf(l2[,1],plot = FALSE)
#get the first order sample autocorrelation of changepoint location k
acf2 = auto_cf_l2_k$acf[2]
acf2
```

so we can obtain that $\rho(X_{1}^{(t-1)},X_{1}^{(t)})=0.05868507$, the by the formula of ESS

$$
T_{ESS} = \frac{1-\rho}{1+\rho}\cdot T
$$

```{r}
T_ESS = round(10000*((1-acf2)/(1+acf2)))
print(paste("thus the effective sample size is", T_ESS))
```

This ESS size is big enough to converge. Therefore, we don't need to run the sampler again for a longer period because the effective sample size is high. This means comparing to i.i.d. sampling, there are not much information loss in the sample from the Markov Chain.

Alternately, we can assess the convergence by cumulative mean average, and the plots showed that the average converges very fast.

```{r}
par(mfrow = c(2, 2))
cum_avg1 <- cummean(l2[,1])
cum_avg2 <- cummean(l2[,2])
cum_avg3 <- cummean(l2[,3])

plot(cum_avg1,type='l',xlab = 'sample',ylab = 'cumulative average',
     main = 'Cumulative means of k')
plot(cum_avg2,type='l',xlab = 'sample',ylab = 'cumulative average',
     main = 'Cumulative means of mu_1')
plot(cum_avg3,type='l',xlab = 'sample',ylab = 'cumulative average',
     main = 'Cumulative means of mu_2')
```

## iii.

Compare the prior and estimated posterior distributions of the three parameters of interest ($k,\mu_{1},\mu_{2}$).

First, plot the posterior vs prior of $k$, the prior of $k$ is also discrete but we plot it as a line with points in order to visualize more clearly,

```{r fig.height=5,fig.width=7}
plot(as.vector(dk2$Var1),as.vector(dk2$Freq/sum(dk2$Freq)),type='h',
     xlim=c(0,150),ylim = c(0,0.045),lwd=2,col='blue',
     main = 'Posterior vs prior of k',xlab = 'Value of k',ylab = 'density')
points(as.vector(dk2$Var1),as.vector(dk2$Freq/sum(dk2$Freq)),col='blue',lwd=1)
lines(prior.k(dataset2),col='green')
points(prior.k(dataset2),col='green')
legend(x = "topright",          
       legend = c("prior", "posterior"),  
       pch = c(1,1),           
       col = c("green", "blue"),          
       lwd = 2)
```

plot the posterior vs prior of $\mu_{1}$,

```{r fig.height=5,fig.width=12}
# Build dataset with different distributions
data2_1 <- data.frame(
  type = c(rep("posterior", 10001), rep("prior", 10000) ),
  value = c(l2[,2], y)
)

p2_1 <- ggplot(data2_1, aes(x=value, fill=type, color=type)) +
    geom_histogram(aes(y=..density..), alpha=0.6, position = 'identity',
                   binwidth = .1) +
    scale_color_manual(values=c("#999999", "#E69F00"))+
    scale_fill_manual(values=c("#999999", "#E69F00"))+
    labs(title="prior vs posterior plot for mu1",x="Value of mu_1", 
         y = "Density")+
    theme_classic()
p2_1
```

plot the posterior vs prior of $\mu_{2}$,

```{r fig.height=5,fig.width=12}
# Build dataset with different distributions
data2_2 <- data.frame(
  type = c( rep("posterior", 10001), rep("prior", 10000) ),
  value = c(l2[,3], y)
)

p2_2 <- ggplot(data2_2, aes(x=value, fill=type, color=type)) +
    geom_histogram(aes(y=..density..), alpha=0.6, position = 'identity',
                   binwidth = .05) +
    scale_color_manual(values=c("#69b3a2", "#404080"))+
    scale_fill_manual(values=c("#69b3a2", "#404080"))+
    labs(title="prior vs posterior plot for mu1",x="Value of mu_2", 
         y = "Density")+
    theme_classic()
p2_2
```

Similarily from these plots, we could clearly see that all posteriors of these three parameters of interest have some differences with their corresponding prior distribution. This means that definitely there is some information in the data and through gibbs sampler we learn more information to update our beliefs.

But this time the posterior distribution of $k$ is not concentrated, it emerges with multiple peaks but finally converge to around 53. So we cannot really determine the change point location till now.

posterior mean and variance of each of the parameters of interest,

```{r}
#posterior mean
mk=mean(l2[,1])
m1=mean(l2[,2])
m2=mean(l2[,3])

#posterior variance
vk=var(l2[,1])
v1=var(l2[,2])
v2=var(l2[,3])

mean_int = c(mk,m1,m2)
var_int = c(vk,v1,v2)
param=c('k','mu_1','mu_2')

param_2 <- data.frame('parameter'=param, "posterior mean"=mean_int, 
                      "posterior variance" = var_int)
param_2
```

Comments:

Compare to data series 1, the posterior mean and variance of $\mu_{1}$ and $\mu_{2}$ are both very small. More precisely, the posterior mean of $\mu_{1}$ of data series 2 is slightly bigger than data series 1. But the posterior mean of $\mu_{2}$ of data series 2 is smaller than data series 1. Their variance of data series 1 and data series 2 are very similar both for $\mu_{1}$ and $\mu_{2}$.

But when we look at the change point location $k$, the variance of data series 2 is very huge but data series 1's is very small. So the mean of data series 2 is not as convinced as data series 1's mean. Even we can think about in data series 2 there maybe not one change point in it. So the result maybe not right.

## (f)

## i.

Assume that we consider a set of 5 models {$\mathcal{M}_{1},\mathcal{M}_{2},\mathcal{M}_{3},\mathcal{M}_{4},\mathcal{M}_{5}$}. $\mathcal{M}_{m}$ means a m changepoints model. $m\in \{1,2,3,4,5\}$ is the index of model and the number of changepoints in this model.

Each model is characterised by a density $f_{m}$ and the associated parameter space $\Theta_{m}$, i.e. $\mathcal{M}_{m} := \{f_{m}(\cdot |\theta), \theta \in \Theta_{m}\}$, where $f_{m}$ is the density and $\Theta_{m}$ the parameter space of the m-changepoints model.

Then we define the parameter space $\Theta_{m}$ more precisely,

$$
\begin{split}
\Theta_{m}:=\{\theta=( \mu_{1},...,\mu_{m+1},k_{1},...,k_{m}  ): 
\\k_{i}\in\{1,2,...,n-1,n\}\;for\;i \in \{1,...,m\}\; and\;k_{1}<k_{2}<...<k_{m},\;
\\\mu_{j}\in \mathbb{R}\;for\;j\in\{1,...,m+1\}  \}
\end{split}
$$

where $n$ is the length of data series.

Using a hierarchical Bayesian setup, we first place a prior distribution of the set of models,

$$
P(\mathcal{M}_{m})=p_{m}=\frac{1}{5}\;\;\;\;\;\;\;with\;\;\;\sum_{m=1}^{5}P(\mathcal{M}_{m})=1
$$

On each parameter space $\Theta_{m}$, we place prior distribution seperately for $\mu_{1},...,\mu_{m+1}$and $k_{1},...,k_{m}$,

For simplicity, we write $\theta=(\hat{\mu},\hat{k})$

$$
\hat{\mu}= (\mu_{1},...,\mu_{i+1})\;\;\;\;\;and\;\;\;\;\; \hat{k}_{m}=(k_{1},...,k_{i})
$$

where $i$ is conditional on the model.

then we place the prior distribution on $\hat{\mu}_{m}$ and $\hat{k}_{m}$ conditional upon their being $m$ changepoints,

the locations of the changepoints is uniform over all possible configurations of m changepoints (conditional upon their being m changepoints), so

$$
P(\hat{k}|\mathcal{M}_{m})=f^{prior}_{m}(\hat{k})=\frac{1}{\mathbf{C}_{m}^{n-1}}
$$

alternatively,

$$
\hat{k}\;|\;\mathcal{M}_{m}\;\thicksim \;Unif(\mathbf{C}_{m}^{n-1})
$$

for the mean parameter $\hat{\mu}$, we use the natural generalisation of the single changepoint model above, then,

$$
f^{prior}_{m}(\hat{\mu})=\prod_{i=1}^{m+1}\phi_{0,10}(\mu_{i})
$$

Assume now that we have observed data $\textbf{y}=(y_{1},...,y_{n})$. We define the likelihood function conditional on the model $\mathcal{M}_{m}$ and parameter $\theta=(\hat{\mu},\hat{k})$ be,

$$
l(\textbf{y}|\theta,\mathcal{M}_{m})=l_{m}(\textbf{y}|\theta)=\prod_{i=1}^{k_{1}}\phi_{\mu_{1},1}(y_{i})\prod_{j=k_{1}+1}^{k_{2}}\phi_{\mu_{2}}(\textbf{y})...\prod_{q=k_{m}+1}^{n}\phi_{\mu_{m+1},1}(\textbf{y})
$$

## ii.

The full posterior distribution over both model and parameters is, (for simplicity we write $f^{post}(\mathcal{M}_{m},\theta)$ as $f^{post}(m,\theta)$)

$$
\begin{split}
f^{post}(m,\theta)=\frac{p_{m} f_{m}^{prior}(\hat{k})f_{m}^{prior}(\hat{\mu}) l_{m}(\textbf{y}|\hat{k},\hat{\mu})}{\sum_{m}^{n-1}p_{m}\int_{\Theta_{m}} f_{m}^{prior}(\hat{k})f_{m}^{prior}(\hat{\mu}) l_{m}(\textbf{y}|\hat{k},\hat{\mu}) \,d\hat{k}\; d\hat{\mu}}
\\=\frac{f_{m}^{prior}(\hat{k})f_{m}^{prior}(\hat{\mu}) l_{m}(\textbf{y}|\hat{k},\hat{\mu})}{\sum_{m}^{n-1}\int_{\Theta_{m}} f_{m}^{prior}(\hat{k})f_{m}^{prior}(\hat{\mu}) l_{m}(\textbf{y}|\hat{k},\hat{\mu}) \,d\hat{k}\; d\hat{\mu}}
\\=\frac{\frac{1}{\mathbf{C}_{5}^{m}}\prod_{i=1}^{m+1}\phi_{0,10}(\mu_{i})\prod_{i=1}^{k_{1}}\phi_{\mu_{1},1}(y_{i})\prod_{j=k_{1}+1}^{k_{2}}\phi_{\mu_{2}}(\textbf{y})...\prod_{q=k_{m}+1}^{n}\phi_{\mu_{m+1},1}(\textbf{y})}{\sum_{m}^{n-1}\int_{\Theta_{m}}\frac{1}{\mathbf{C}_{5}^{m}}\prod_{i=1}^{m+1}\phi_{0,10}(\mu_{i})\prod_{i=1}^{k_{1}}\phi_{\mu_{1},1}(y_{i})\prod_{j=k_{1}+1}^{k_{2}}\phi_{\mu_{2}}(\textbf{y})...\prod_{q=k_{m}+1}^{n}\phi_{\mu_{m+1},1}(\textbf{y})\,d\hat{k}\; d\hat{\mu}}
\end{split}
$$

defined on the disjoint union space

$$
\Theta:=\bigcup_{m=1}^{5}(\{m\}\times\Theta_{m})
$$

## iii.

Consider the $1\to 2$ move first, we draw the mean $\mu_{3}$ and change point location $k_{2}$ of the new component from the corresponding prior. And we need to adjust the prior of the change point location a bit for $k_{2}$. So we draw $k_{2}$ uniformly from $\{1,...,k_{1}-1,k_{1}+1,...,n-1\}$ which not included $k_{1}$. Therefore, we draw,

$$
u_{1}^{(t-1)}\;\sim\;g_{1}\;\;\;\;\;\;\;\;\;u_{2}^{(t-1)}\;\sim\;g_{2}
$$

and set

$$
\mu_{3}=u_{1}^{(t-1)}\;\;\;\;\;\;\;\;\;k_{2}=u_{2}^{(t-1)}
$$

with $g_{1}$ being the density of the prior distribution on the $\mu_{2}$, $g_{2}$ being the density of uniform distribution on the discrete set $\{1,...,k_{1}-1,k_{1}+1,...,n-1\}$.

The corresponding transformations $T_{1\to 2}$ is,

$$
T_{1\to 2}\begin{pmatrix}
\mu_{1}^{(t-1)}\\
\mu_{2}^{(t-1)}\\
k_{1}^{(t-1)}\\
u_{1}^{(t-1)}\\
u_{2}^{(t-1)}
\end{pmatrix}=
\begin{pmatrix}
\mu_{1}^{(t-1)}\\
\mu_{2}^{(t-1)}\\
u_{1}^{(t-1)}\\
k_{1}^{(t-1)}\\
u_{2}^{(t-1)}
\end{pmatrix}=
\begin{pmatrix}
\mu_{1}\\
\mu_{2}\\
\mu_{3}\\
k_{1}\\
k_{2}
\end{pmatrix}
$$

The determinant of the Jacobian of $T_{1\to 2}$ is 1.

In order to include fixed-dimensional moves, we use the setup corresponds to,

$$
\rho_{1\to 1}=\frac{1}{2},\;\rho_{1\to 2}=\frac{1}{2},\;\rho_{2\to 1}=\frac{1}{2},\;\rho_{2\to 2}=\frac{1}{2}
$$

-- If the current model is single changepoint model $\mathcal{M}_{1}$ (i.e. m\^{t-1}=1)

-   With probability $\frac{1}{2}$ perform an update of $\theta^{(t-1)}=(\mu_{1}^{(t-1)},\mu_{2}^{(t-1)},k_{1}^{(t-1)})$ within model $\mathcal{M}_{1}$, i.e.

1.  Generate $\theta$ by its components' full conditional distribution

$$
\begin{split}
\mu_{1}^{(t)} \sim f_{1}^{post}(\mu_{1}|\mu^{(t-1)}_{2},k_{1}^{(t-1)})\\
\mu_{2}^{(t)} \sim f_{1}^{post}(\mu_{2}|\mu^{(t)}_{1},k_{1}^{(t-1)})\\
k_{1}^{(t)} \sim f_{1}^{post}(k|\mu^{(t)}_{1},\mu^{(t)}_{2})
\end{split}
$$

2.  Set $\theta^{(t)}=\theta=(\mu_{1}^{(t)},\mu_{2}^{(t)},k_{1}^{(t)})$ .(Gibbs sampler is a special case of Metropolis-Hastings with acceptance probability 1.)

-   Otherwise(with probability $\frac{1}{2}$), attempt a jump to model $\mathcal{M}_{2}$,i.e.

1.  Generate $u_{1}^{(t-1)}\;\sim\;g_{1}$ and $u_{2}^{(t-1)}\;\sim\;g_{2}$ ($g_{1}$ and $g_{2}$ as we introduced above).

2.  Set

$$
\begin{pmatrix}
\mu_{1}\\
\mu_{2}\\
\mu_{3}\\
k_{1}\\
k_{2}
\end{pmatrix}
=T_{1\to 2}\begin{pmatrix}
\mu_{1}^{(t-1)}\\
\mu_{2}^{(t-1)}\\
k_{1}^{(t-1)}\\
u_{1}^{(t-1)}\\
u_{2}^{(t-1)}
\end{pmatrix}=
\begin{pmatrix}
\mu_{1}^{(t-1)}\\
\mu_{2}^{(t-1)}\\
u_{1}^{(t-1)}\\
k_{1}^{(t-1)}\\
u_{2}^{(t-1)}
\end{pmatrix}
$$

3.  Compute,($\frac{1}{2}\times \frac{1}{3}$ in the probability of picking one $\mu$ and one $k$ from 3 $\mu$ and 2 $k$ in the $2\to 1$ step)

$$
\begin{split}
\alpha = min\{1,\frac{p_{2}f^{prior}_{2}(k_{1},k_{2})f^{prior}_{2}(\mu_{1},\mu_{2},\mu_{3})l_{2}(y_{1},...,y_{n}|\mu_{1},\mu_{2},\mu_{3},k_{1},k_{2})}{p_{1}f^{prior}_{1}(k_{1}^{(t-1)})f^{prior}_{1}(\mu_{1}^{(t-1)},\mu_{2}^{(t-1)})l_{1}(y_{1},...,y_{n}|\mu_{1}^{(t-1)},\mu_{2}^{(t-1)},k_{1}^{(t-1)})}\cdot \frac{\rho_{2\to 1}}{\rho_{1\to 2}}\cdot \frac{\frac{1}{2}\times \frac{1}{3}}{g_{1}(u^{(t-1)}_{1})\cdot g_{2}(u^{(t-1)}_{2})}\cdot 1  \}\\
=min\{1,\frac{(n-1)\prod_{i=1}^{3}\phi_{0,10}(\mu_{i})l_{2}(y_{1},...,y_{n}|\mu_{1},\mu_{2},\mu_{3},k_{1},k_{2})}{(\mathbf{C}^{n-1}_{2})\prod_{i=1}^{2}\phi_{0,10}(\mu_{i}^{(t-1)})l_{1}(y_{1},...,y_{n}|\mu_{1}^{(t-1)},\mu_{2}^{(t-1)},k_{1}^{(t-1)})}\cdot \frac{1}{6\cdot g_{1}(u^{(t-1)}_{1})\cdot g_{2}(u^{(t-1)}_{2})} \}
\end{split}
$$

4.  With probability $\alpha$, we set $m^{(t)}=2$ and $\theta^{(t)}=(\mu_{1}^{(t)},\mu_{2}^{(t)},\mu_{3}^{(t)},k_{1}^{(t)},k_{2}^{(t)})$. Otherwise, remain $m^{t}=1$ and $\theta^{(t)}=\theta^{(t-1)}$.

-- If the current model is two changepoint model $\mathcal{M}_{2}$ (i.e. m\^{t-1}=2)

-   With probability $\frac{1}{2}$ perform an update of $\theta^{(t-1)}=(\mu_{1}^{(t-1)},\mu_{2}^{(t-1)},\mu_{3}^{(t-1)},k_{1}^{(t-1)},k_{2}^{(t-1)})$ within model $\mathcal{M}_{2}$, i.e.

1.  Generate $\theta$ by its components' full conditional distribution

$$
\begin{split}
\mu_{1}^{(t)} \sim f_{2}^{post}(\mu_{1}|\mu^{(t-1)}_{2},\mu_{3}^{(t-1)},k_{1}^{(t-1)},k_{2}^{(t-1)})\\
\mu_{2}^{(t)} \sim f_{2}^{post}(\mu_{2}|\mu^{(t)}_{1},\mu_{3}^{(t-1)},k_{1}^{(t-1)},k_{2}^{(t-1)})\\
\mu_{3}^{(t)} \sim f_{2}^{post}(\mu_{3}|\mu^{(t)}_{1},\mu_{2}^{(t)},k_{1}^{(t-1)},k_{2}^{(t-1)})\\
k_{1}^{(t)} \sim f_{2}^{post}(k|\mu^{(t)}_{1},\mu^{(t)}_{2},\mu_{3}^{(t)},k^{(t-1)}_{2})\\
k_{2}^{(t)} \sim f_{2}^{post}(k|\mu^{(t)}_{1},\mu^{(t)}_{2},\mu_{3}^{(t)},k^{(t)}_{1})
\end{split}
$$

2.  Set $\theta^{(t)}=\theta=(\mu_{1}^{(t)},\mu_{2}^{(t)},\mu_{3}^{(t)},k_{1}^{(t)},k_{2}^{(t)})$ .(Gibbs sampler is a special case of Metropolis-Hastings with acceptance probability 1.)

-   Otherwise(with probability $\frac{1}{2}$), attempt a jump to model $\mathcal{M}_{2}$,i.e.

1.  Drop one $\mu$ randomly with probability $\frac{1}{3}$ from $\{\mu^{(t-1)}_{1},\mu^{(t-1)}_{2},\mu_{3}^{(t-1)}\}$.

2.  Drop one $k$ randomly with probability $\frac{1}{2}$ from $\{k^{(t-1)}_{1},k^{(t-1)}_{2}\}$.

3.  Compute the reciprocal

$$
\frac{1}{\alpha}
$$

4.  With probability $\frac{1}{\alpha}$, we set $m^{(t)}=1$ and $\theta^{(t)}=(\mu_{1}^{(t)},\mu_{2}^{(t)},k_{1}^{(t)})$. Otherwise, remain $m^{t}=2$ and $\theta^{(t)}=\theta^{(t-1)}$.

## iv.

We remain $\rho_{2\to 1}$ and $\rho_{1\to 2}$ in the acceptance probability,

$$
\begin{split}
\alpha = min\{1,\frac{p_{2}f^{prior}_{2}(k_{1},k_{2})f^{prior}_{2}(\mu_{1},\mu_{2},\mu_{3})l_{2}(y_{1},...,y_{n}|\mu_{1},\mu_{2},\mu_{3},k_{1},k_{2})}{p_{1}f^{prior}_{1}(k_{1}^{(t-1)})f^{prior}_{1}(\mu_{1}^{(t-1)},\mu_{2}^{(t-1)})l_{1}(y_{1},...,y_{n}|\mu_{1}^{(t-1)},\mu_{2}^{(t-1)},k_{1}^{(t-1)})}\cdot \frac{\rho_{2\to 1}}{\rho_{1\to 2}}\cdot \frac{\frac{1}{2}\times \frac{1}{3}}{g_{1}(u^{(t-1)}_{1})\cdot g_{2}(u^{(t-1)}_{2})}\cdot 1\}\\
=min\{1,\frac{(n-1)\prod_{i=1}^{3}\phi_{0,10}(\mu_{i})l_{2}(y_{1},...,y_{n}|\mu_{1},\mu_{2},\mu_{3},k_{1},k_{2})}{(\mathbf{C}^{n-1}_{2})\prod_{i=1}^{2}\phi_{0,10}(\mu_{i}^{(t-1)})l_{1}(y_{1},...,y_{n}|\mu_{1}^{(t-1)},\mu_{2}^{(t-1)},k_{1}^{(t-1)})}\cdot \frac{\rho_{2\to 1}}{\rho_{1\to 2}}\cdot \frac{1}{6\cdot g_{1}(u^{(t-1)}_{1})\cdot g_{2}(u^{(t-1)}_{2})}\}
\end{split}
$$
